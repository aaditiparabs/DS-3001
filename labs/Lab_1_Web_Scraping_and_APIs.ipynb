{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hkkb_7LYKsG4"
   },
   "source": [
    "## **Name:** First Last\n",
    "\n",
    "## **Computing ID:** ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1DAOPfwqMV5t"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time, requests, pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Web Scraping & APIs (100 pts)\n",
    "## Due September 22nd at 11:59PM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1iwoJjEGsY2ej0s9i0iPXw50qpjNoDf_p?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4A9sts1JrTc"
   },
   "source": [
    "\n",
    "A great source of data and Pandas practice is getting data from the Internet. Rather than a .csv file, many data will be in a stream of records, typically in XML (eXtensible Mark-up Language) or JSON (JavaScript Object Notation) format.\n",
    "\n",
    "## Overview\n",
    "\n",
    "You will collect data three ways and assemble it into Pandas DataFrames:\n",
    "\n",
    "1. Scrape a Wikipedia page for two different HTML tags (20 pts total)\n",
    "\n",
    "2. Scrape any non-Wikipedia HTML page for a (different) tag of interest (40 pts)\n",
    "\n",
    "3. Use the PokeAPI to collect data using 5+ API queries (40 pts)\n",
    "\n",
    "## Deliverable\n",
    "For each section, put collected data into a Pandas DataFrame.\n",
    "\n",
    "**Exception:** If a required tag’s content is only available inside an image (e.g., charts, logos),  plot the image URL(s) directly instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec98NMgiSt2T"
   },
   "source": [
    "# **Part 1: Scrape a Wikipedia page for two different tags (20 pts)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xDGVFPlwbLAC"
   },
   "source": [
    "In Part 1, you will pick any single Wikipedia article and extract two distinct HTML tag types (ie: H1, H2, p, a, img). You will create a DataFrame per tag type and display the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqMNWV3-bYJK"
   },
   "source": [
    "## **Question 1 (5 points):** Pick a Wikipedia URL to web scrape data from. Decide which 2 HTML tags you would like to scrape (a comprehensive list is above).\n",
    "\n",
    "Save the URL and tags as string and write them in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "92s4GX0CbHQU"
   },
   "outputs": [],
   "source": [
    "url_str = \"https://en.wikipedia.org/wiki/Quokka\"\n",
    "tag1 = \"h1\"\n",
    "tag2 = \"img\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78SusRUdeVut"
   },
   "source": [
    "\n",
    "## **Question 2 (5 points):** Write code to download your initial raw data using the `requests` package and store it in a variable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9WrxAuLQcJJo"
   },
   "outputs": [],
   "source": [
    "# Code to extract raw data here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kFT5NiQAurMg"
   },
   "source": [
    "\n",
    "## **Question 3 (10 points):** Extract your 2 chosen tags from your raw data and display the results. If the tag is text-based, create a DataFrame for the resulting tag contents. If the tag is image-based, plot 1 to 3 of your resulting images in the same figure.\n",
    "\n",
    "If both tags are text based, you will create a separate DataFrame per tag.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QJ1dpOJgpyQz"
   },
   "outputs": [],
   "source": [
    "# Code to display your DataFrame(s) or images here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-fZ91kL4c42C"
   },
   "source": [
    "# **Part 2: Scrape a Non-Wikipedia page (40 pts)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lE789VwIT9kK"
   },
   "source": [
    "**Web Scraping Examples**\n",
    "* https://toscrape.com/\n",
    "* https://www.scrapethissite.com/\n",
    "* https://github.com/stanfordjournalism/search-script-scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-8vEftBdGXa"
   },
   "source": [
    "In Part 2, you will choose any publicly accessible page (NOT Wikipedia) and build a DataFrame with at least 10 or more rows. The structure of the DataFrame and what tag(s) you extract will be your choice.\n",
    "\n",
    "Examples of targets: a university course listing page, a simple product listing, a blog archive page, a conference schedule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXjd7_cbdhUX"
   },
   "source": [
    "\n",
    "## **Question 1 (10 points):** Write code to connect to your chosen URL, download your initial raw data using the `requests` package, and store it in a variable. Decide which tag(s) you will scrape and store them in the list, mytags.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V9VMZ_lidmJg"
   },
   "outputs": [],
   "source": [
    "url = \"https://books.toscrape.com/\"\n",
    "mytags = [\"p\", \"a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "whEPEwvZd1bc"
   },
   "outputs": [],
   "source": [
    "# Code to extract raw data here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SRHLrn3dd2Oc"
   },
   "source": [
    "\n",
    "## **Question 2 (20 points):** Extract your chosen tag(s) from your raw data and display the results. Like before, create a DataFrame for text based tags. If the tag is image-based, plot 1 to 3 of your resulting images in the same figure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6C9IgCqOeuyC"
   },
   "outputs": [],
   "source": [
    "# Code to extract raw data here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNkLdvy2eyKB"
   },
   "source": [
    "\n",
    "## **Question 3 (10 points):** Mine one or more of your resulting DataFrames to answer a question. Questions may be quantitative or qualitative. Here are a few examples to get you started, based on the books storefront example link.\n",
    "\n",
    "\n",
    "*Quantitative*\n",
    "*   What is the average book price?\n",
    "*   What is the range (minimum and maximum) of book prices?\n",
    "*   What is the most and least expensive book?\n",
    "*   How many books are 5 star rated?\n",
    "\n",
    "*Qualitative*\n",
    "*   What are the 5 star books?\n",
    "*   What are the images of 5 star books? How do they compare to the 1 star books?\n",
    "*  What factors contribute to if a book is priced high or low?\n",
    "* etc...\n",
    "\n",
    "## Discuss in 2 or more sentences your interpretation of the answer(s) to your question(s).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rU1PcpVgfnpj"
   },
   "outputs": [],
   "source": [
    "my_question = \"What is the average book price?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rn_7YwIhfqf3"
   },
   "outputs": [],
   "source": [
    "# Code to answer your question here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOx94Eb6f4AU"
   },
   "source": [
    "**The results to my question were....**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ff6-I_xEdyQn"
   },
   "source": [
    "# **Part 3: Using an API for Data Sourcing (40 pts)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhI_ABrwgH8O"
   },
   "source": [
    "In Part 3, you will query at least 5 Pokémon from the PokeAPI and build a DataFrame containing 5 or more columns that are interesting and comparable across Pokémon. Sample characteristics are below. If desired, another equivalent API of your choice can replace PokeAPI so long as the resulting DataFrame includes 5 or more columns.\n",
    "\n",
    "**Example Characteristics:**\n",
    "*   name\n",
    "*   id\n",
    "* base_experience\n",
    "* height\n",
    "* weight\n",
    "* types (comma-joined)\n",
    "* abilities (count)\n",
    "* Statistics like HP/Attack\n",
    "* Sprite URL (!!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXjn9TF8UPBT"
   },
   "source": [
    "\n",
    "**Alternative API Examples**\n",
    "*   https://pokeapi.co/\n",
    "  * Example: https://pokeapi.co/api/v2/pokemon?limit=1000&offset=0\n",
    "  * Example: https://pokeapi.co/api/v2/pokemon/ditto\n",
    "  * Example: https://pokeapi.co/api/v2/pokemon/pikachu\n",
    "* https://place.dog/\n",
    "* https://dukengn.github.io/Dog-facts-API/  \n",
    "* https://apiv3.iucnredlist.org/api/v3/docs\n",
    "\n",
    "A more exhaustive API list can be found at: https://github.com/public-apis/public-apis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CTK2ds0wg14g"
   },
   "source": [
    "## **Question 1 (5 points):** Write below the API URLs you will scrape using the requests package. Create a reusable header dictionary and pass it through your requests.get connection. Write a sentence below on which parameter you will vary and which features (5+) you expect to scrape.\n",
    "**Before starting, make a few manual connections the API using your browser and assess the output for any characteristics you might want to scrape. You can use the URL a few times to assess which parameter(s) will be your variable (ie: Pokemon Name for pokeapi).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1WycXAkyg1wt"
   },
   "outputs": [],
   "source": [
    "my_api_urls = [\"https://pokeapi.co/api/v2/pokemon/ditto\", \"https://pokeapi.co/api/v2/pokemon/pikachu\"]\n",
    "api_header = {\"User-Agent\":\"Mozilla/5.0 (student-lab)\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IyOvO2iah-_m"
   },
   "source": [
    "**I plan on scraping the following features that will be my columns for my DataFrame: Pokemon Name, HP, ....**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHL2xAgUwbwr"
   },
   "source": [
    "## **Question 2 (25 points):** Create the DataFrame of your selected feature(s). You might need to implement loops to extract enough clean features to use. Be creative with using iteration and data structures to get your data!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jy3rqs9Ub15G",
    "outputId": "55f453a0-e09e-4624-e91d-ddccddc53796"
   },
   "outputs": [],
   "source": [
    "names = [\"ditto\", \"snorlax\", \"pikachu\", \"bulbasaur\", \"charmander\"]  # At least 5\n",
    "autocomplete_urls = [\"https://pokeapi.co/api/v2/pokemon/\" + x for x in names]\n",
    "\n",
    "rows = []\n",
    "for url in autocomplete_urls:\n",
    "  print(url)\n",
    "  # Add requests code per iteration\n",
    "  # Ensure you are saving data in each iteration somehow to build a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUuESO7ujoXA"
   },
   "source": [
    "## **Question 3 (10 points):** Display your resulting DataFrame. Create 3 plots of your choice that summarize the features you've collected.\n",
    "* If you've scraped quantitative data, plot a few ranges or distributions of data\n",
    "* If you've scraped any image URLs, plot a few of them\n",
    "\n",
    "**Optional extensions**\n",
    "* If you've scraped any image URLs, add a column for file extension type\n",
    "* Run some value counts on different variables\n",
    "* If your columns are messy or hard to read, rename them\n",
    "* Cast data types into a usable state (ie: string to integer for numerical analysis)\n",
    "\n",
    "## After exploring, write down 1 to 2 sentences about any interestings aspects of your data (group characteristics, clear relationships between variables) or your process of getting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LLQ5j3YPlyt_"
   },
   "outputs": [],
   "source": [
    "# Code to plot features here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J72n7H18jm0w"
   },
   "source": [
    "**The results of my DataFrame show that Pokemon that have....**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1KiVMvjNKGU"
   },
   "source": [
    "### Honor Pledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jo-hwQo5LBAk"
   },
   "source": [
    "On my honor as a student, I have neither given nor received unauthorized aid on this assignment."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
